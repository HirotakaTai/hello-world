# ğŸ“¦ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆäº‹å‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
# pip install langgraph==0.4.8 langchain openai google-generativeai pydantic

import os
from typing import Optional

import google.generativeai as genai
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from langchain.schema import SystemMessage
from langgraph.graph import END
from langgraph.graph import StateGraph
from pydantic import BaseModel

# ğŸŒ API ã‚­ãƒ¼ã®è¨­å®š
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# ğŸŒŸ LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
openai_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
genai.configure(api_key=GEMINI_API_KEY)
gemini_llm = genai.GenerativeModel("gemini-pro")


# ğŸ§  çŠ¶æ…‹å®šç¾©
class LLMCompareState(BaseModel):
    question: str
    openai_response: Optional[str] = None
    gemini_response: Optional[str] = None
    merged_summary: Optional[str] = None


# ğŸ¤– OpenAI ã«å•ã„åˆã‚ã›ã‚‹ãƒãƒ¼ãƒ‰
def ask_openai(state: LLMCompareState) -> LLMCompareState:
    messages = [
        SystemMessage(
            content="ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å‘½é¡Œã«ã¤ã„ã¦ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        ),
        HumanMessage(content=state.question),
    ]
    response = openai_llm(messages)
    return state.copy(update={"openai_response": response.content.strip()})


# âœ¨ Gemini ã«å•ã„åˆã‚ã›ã‚‹ãƒãƒ¼ãƒ‰
def ask_gemini(state: LLMCompareState) -> LLMCompareState:
    response = gemini_llm.generate_content(state.question)
    return state.copy(update={"gemini_response": response.text.strip()})


# ğŸ”— å›ç­”ã‚’çµ±åˆãƒ»è¦ç´„ã™ã‚‹ãƒãƒ¼ãƒ‰ï¼ˆOpenAI ä½¿ç”¨ï¼‰
def merge_responses(state: LLMCompareState) -> LLMCompareState:
    messages = [
        SystemMessage(
            content="ä»¥ä¸‹ã®è¤‡æ•°ã®å›ç­”ã‚’å‚è€ƒã«ã€çŸ›ç›¾ãŒãªã„ã‚ˆã†ã«çµ±åˆã•ã‚ŒãŸç°¡æ½”ã§æ­£ç¢ºãªå›ç­”ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        ),
        HumanMessage(
            content=f"OpenAIã®å›ç­”: {state.openai_response}\nGeminiã®å›ç­”: {state.gemini_response}"
        ),
    ]
    summary = openai_llm(messages)
    return state.copy(update={"merged_summary": summary.content.strip()})


# ğŸ–¨ï¸ çµæœå‡ºåŠ›ãƒãƒ¼ãƒ‰
def output_result(state: LLMCompareState) -> LLMCompareState:
    print("\nğŸ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:")
    print(state.question)
    print("\nğŸ¤– OpenAIã®å›ç­”:")
    print(state.openai_response)
    print("\nâœ¨ Geminiã®å›ç­”:")
    print(state.gemini_response)
    print("\nğŸ”— çµ±åˆè¦ç´„:")
    print(state.merged_summary)
    return state


# ğŸ›  ã‚°ãƒ©ãƒ•æ§‹ç¯‰
builder = StateGraph(LLMCompareState)

builder.add_node("AskOpenAI", ask_openai)
builder.add_node("AskGemini", ask_gemini)
builder.add_node("Merge", merge_responses)
builder.add_node("Output", output_result)

builder.set_entry_point("FanOut")

# FanOut ãƒãƒ¼ãƒ‰ã¯å®Ÿéš›ã®å‡¦ç†ã‚’ã—ãªã„ä¸­ç¶™ãƒãƒ¼ãƒ‰
builder.add_node("FanOut", lambda state: state)
builder.add_edge("FanOut", "AskOpenAI")
builder.add_edge("FanOut", "AskGemini")
builder.add_edge("AskOpenAI", "Merge")
builder.add_edge("AskGemini", "Merge")
builder.add_edge("Merge", "Output")
builder.add_edge("Output", END)

# ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
graph = builder.compile()

# ğŸš€ CLI å®Ÿè¡Œ
if __name__ == "__main__":
    print("\nğŸ” è¤‡æ•°LLMã«ã‚ˆã‚‹è³ªå•å¿œç­”ã¨è¦ç´„ï¼ˆLangGraph + OpenAI + Geminiï¼‰")
    question = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    state = LLMCompareState(question=question)
    graph.invoke(state)
    print("\nâœ… å®Œäº†")
