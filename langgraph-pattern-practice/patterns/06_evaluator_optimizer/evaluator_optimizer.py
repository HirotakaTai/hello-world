"""
Evaluator-optimizer ãƒ‘ã‚¿ãƒ¼ãƒ³
===========================

ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€ä¸€ã¤ã®LLMãŒå›ç­”ã‚’ç”Ÿæˆã—ã€åˆ¥ã®LLMãŒãã®å›ç­”ã‚’è©•ä¾¡ã—ã¦
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã€å…ƒã®LLMãŒæ”¹å–„ç‰ˆã‚’ä½œæˆã™ã‚‹ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

ç‰¹å¾´ï¼š
- Generatorï¼ˆç”Ÿæˆè€…ï¼‰: åˆæœŸå›ç­”ã‚’ä½œæˆ
- Evaluatorï¼ˆè©•ä¾¡è€…ï¼‰: å›ç­”ã®å“è³ªã‚’è©•ä¾¡ã—ã€æ”¹å–„ç‚¹ã‚’ç‰¹å®š
- Optimizerï¼ˆæœ€é©åŒ–è€…ï¼‰: è©•ä¾¡ã‚’åŸºã«æ”¹å–„ç‰ˆã‚’ä½œæˆ
- åå¾©ãƒ—ãƒ­ã‚»ã‚¹: æº€è¶³ã®ã„ãå“è³ªã«ãªã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—

ä¾‹ï¼š
- æ–‡å­¦ç¿»è¨³ã®å“è³ªå‘ä¸Š
- è¤‡é›‘ãªæ¤œç´¢ã‚¿ã‚¹ã‚¯ã§ã®æƒ…å ±ç²¾åº¦å‘ä¸Š
- ã‚³ãƒ¼ãƒ‰ã®å“è³ªæ”¹å–„
- æ–‡ç« ã®æ¨æ•²ã¨æ”¹å–„

ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ©ç‚¹ï¼š
- åå¾©çš„ãªå“è³ªå‘ä¸Š
- å®¢è¦³çš„ãªè©•ä¾¡ã«ã‚ˆã‚‹æ”¹å–„
- äººé–“ã®æ¨æ•²ãƒ—ãƒ­ã‚»ã‚¹ã®æ¨¡å€£
- é«˜å“è³ªãªæœ€çµ‚æˆæœç‰©ã®ç²å¾—
"""

import time
from datetime import datetime
from typing import Any
from typing import Dict

from dotenv import load_dotenv
from langchain.schema import HumanMessage
from langchain.schema import SystemMessage
from langchain_openai import ChatOpenAI

# ===== ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ =====
load_dotenv()


class EvaluationCriteria:
    """
    è©•ä¾¡åŸºæº–ã‚’å®šç¾©ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    # ===== ç¿»è¨³å“è³ªè©•ä¾¡åŸºæº– =====
    TRANSLATION_QUALITY = {
        "accuracy": "åŸæ–‡ã®æ„å‘³ã®æ­£ç¢ºæ€§",
        "fluency": "è‡ªç„¶ã•ã¨èª­ã¿ã‚„ã™ã•",
        "completeness": "å†…å®¹ã®å®Œå…¨æ€§",
        "cultural_adaptation": "æ–‡åŒ–çš„é©å¿œæ€§",
        "terminology": "å°‚é–€ç”¨èªã®é©åˆ‡æ€§",
    }

    # ===== æ–‡ç« å“è³ªè©•ä¾¡åŸºæº– =====
    WRITING_QUALITY = {
        "clarity": "æ˜ç¢ºæ€§ã¨ç†è§£ã—ã‚„ã™ã•",
        "coherence": "è«–ç†çš„ãªä¸€è²«æ€§",
        "engagement": "èª­è€…ã®é–¢å¿ƒã‚’å¼•ãåº¦åˆã„",
        "grammar": "æ–‡æ³•çš„æ­£ç¢ºæ€§",
        "style": "æ–‡ä½“ã®é©åˆ‡æ€§",
    }

    # ===== ã‚³ãƒ¼ãƒ‰å“è³ªè©•ä¾¡åŸºæº– =====
    CODE_QUALITY = {
        "functionality": "æ©Ÿèƒ½ã®æ­£ç¢ºæ€§",
        "readability": "å¯èª­æ€§",
        "efficiency": "åŠ¹ç‡æ€§",
        "maintainability": "ä¿å®ˆæ€§",
        "security": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£",
    }

    # ===== ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆè©•ä¾¡åŸºæº– =====
    RESEARCH_QUALITY = {
        "accuracy": "æƒ…å ±ã®æ­£ç¢ºæ€§",
        "completeness": "åŒ…æ‹¬æ€§",
        "methodology": "èª¿æŸ»æ–¹æ³•ã®é©åˆ‡æ€§",
        "analysis": "åˆ†æã®æ·±ã•",
        "presentation": "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è³ª",
    }


class Generator:
    """
    ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚’æ‹…å½“ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.7,  # å‰µé€ çš„ãªç”Ÿæˆã®ãŸã‚é©åº¦ãªãƒ©ãƒ³ãƒ€ãƒ æ€§
        )
        self.generation_count = 0

    def generate_initial_response(self, task: str, context: str = "") -> Dict[str, Any]:
        """
        åˆæœŸå›ç­”ã‚’ç”Ÿæˆ

        Args:
            task (str): å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯
            context (str): è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±

        Returns:
            Dict[str, Any]: ç”Ÿæˆçµæœ
        """

        start_time = time.time()
        self.generation_count += 1

        print("âœï¸ Generator: åˆæœŸå›ç­”ã‚’ç”Ÿæˆä¸­...")

        system_prompt = """
        ã‚ãªãŸã¯é«˜å“è³ªãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã®å°‚é–€å®¶ã§ã™ã€‚
        ä¸ãˆã‚‰ã‚ŒãŸã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã€æœ€å–„ã‚’å°½ãã—ã¦å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        åˆå›ç”Ÿæˆãªã®ã§ã€å¯èƒ½ãªé™ã‚ŠåŒ…æ‹¬çš„ã§é«˜å“è³ªãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
        """

        user_prompt = f"""
        ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
        
        ã€ã‚¿ã‚¹ã‚¯ã€‘
        {task}
        """

        if context:
            user_prompt += f"\n\nã€è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{context}"

        user_prompt += """
        
        é«˜å“è³ªã§å®Œå…¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        """

        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]

            response = self.llm.invoke(messages)
            execution_time = time.time() - start_time

            print(f"âœ… Generator: åˆæœŸå›ç­”ç”Ÿæˆå®Œäº† ({execution_time:.2f}ç§’)")

            return {
                "success": True,
                "content": response.content,
                "generation_type": "initial",
                "generation_number": self.generation_count,
                "execution_time": execution_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ Generator: åˆæœŸå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

            return {
                "success": False,
                "error": str(e),
                "generation_type": "initial",
                "generation_number": self.generation_count,
                "execution_time": execution_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

    def generate_improved_response(
        self,
        original_task: str,
        previous_response: str,
        evaluation_feedback: str,
        context: str = "",
    ) -> Dict[str, Any]:
        """
        è©•ä¾¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åŸºã«æ”¹å–„ã•ã‚ŒãŸå›ç­”ã‚’ç”Ÿæˆ

        Args:
            original_task (str): å…ƒã®ã‚¿ã‚¹ã‚¯
            previous_response (str): å‰ã®å›ç­”
            evaluation_feedback (str): è©•ä¾¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            context (str): è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±

        Returns:
            Dict[str, Any]: æ”¹å–„ã•ã‚ŒãŸç”Ÿæˆçµæœ
        """

        start_time = time.time()
        self.generation_count += 1

        print(f"ğŸ”„ Generator: æ”¹å–„å›ç­”ã‚’ç”Ÿæˆä¸­ï¼ˆ{self.generation_count}å›ç›®ï¼‰...")

        system_prompt = """
        ã‚ãªãŸã¯ç¶™ç¶šçš„æ”¹å–„ã®å°‚é–€å®¶ã§ã™ã€‚
        è©•ä¾¡è€…ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åŸºã«ã€å‰å›ã®å›ç­”ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚
        æŒ‡æ‘˜ã•ã‚ŒãŸå•é¡Œç‚¹ã‚’ä¿®æ­£ã—ã€ã‚ˆã‚Šé«˜å“è³ªãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        """

        user_prompt = f"""
        ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€æ”¹å–„ã•ã‚ŒãŸå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
        
        ã€å…ƒã®ã‚¿ã‚¹ã‚¯ã€‘
        {original_task}
        
        ã€å‰å›ã®å›ç­”ã€‘
        {previous_response}
        
        ã€è©•ä¾¡è€…ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€‘
        {evaluation_feedback}
        """

        if context:
            user_prompt += f"\n\nã€è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{context}"

        user_prompt += """
        
        è©•ä¾¡è€…ã®æŒ‡æ‘˜ã‚’çœŸæ‘¯ã«å—ã‘æ­¢ã‚ã€å…·ä½“çš„ãªæ”¹å–„ã‚’è¡Œã£ãŸå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        å‰å›ã®è‰¯ã„éƒ¨åˆ†ã¯ç¶­æŒã—ã¤ã¤ã€å•é¡Œç‚¹ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
        """

        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]

            response = self.llm.invoke(messages)
            execution_time = time.time() - start_time

            print(f"âœ… Generator: æ”¹å–„å›ç­”ç”Ÿæˆå®Œäº† ({execution_time:.2f}ç§’)")

            return {
                "success": True,
                "content": response.content,
                "generation_type": "improved",
                "generation_number": self.generation_count,
                "execution_time": execution_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ Generator: æ”¹å–„å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

            return {
                "success": False,
                "error": str(e),
                "generation_type": "improved",
                "generation_number": self.generation_count,
                "execution_time": execution_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }


class Evaluator:
    """
    ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è©•ä¾¡ã‚’æ‹…å½“ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.2,  # è©•ä¾¡ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ä½æ¸©åº¦
        )
        self.evaluation_count = 0

    def evaluate_response(
        self,
        task: str,
        response: str,
        criteria: Dict[str, str],
        quality_threshold: float = 7.0,
    ) -> Dict[str, Any]:
        """
        å›ç­”ã‚’è©•ä¾¡

        Args:
            task (str): å…ƒã®ã‚¿ã‚¹ã‚¯
            response (str): è©•ä¾¡ã™ã‚‹å›ç­”
            criteria (Dict[str, str]): è©•ä¾¡åŸºæº–
            quality_threshold (float): å“è³ªé–¾å€¤ï¼ˆ10ç‚¹æº€ç‚¹ï¼‰

        Returns:
            Dict[str, Any]: è©•ä¾¡çµæœ
        """

        start_time = time.time()
        self.evaluation_count += 1

        print(f"ğŸ” Evaluator: å›ç­”ã‚’è©•ä¾¡ä¸­ï¼ˆ{self.evaluation_count}å›ç›®ï¼‰...")

        # ===== è©•ä¾¡åŸºæº–ã‚’æ–‡å­—åˆ—åŒ– =====
        criteria_text = "\n".join(
            [f"- {key}: {description}" for key, description in criteria.items()]
        )

        system_prompt = f"""
        ã‚ãªãŸã¯å³æ ¼ã§å…¬æ­£ãªè©•ä¾¡è€…ã§ã™ã€‚
        ä»¥ä¸‹ã®è©•ä¾¡åŸºæº–ã«å¾“ã£ã¦ã€å›ç­”ã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        
        ã€è©•ä¾¡åŸºæº–ã€‘
        {criteria_text}
        
        å„åŸºæº–ã«ã¤ã„ã¦1-10ç‚¹ã§è©•ä¾¡ã—ã€å»ºè¨­çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        """

        user_prompt = f"""
        ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã¨å›ç­”ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
        
        ã€å…ƒã®ã‚¿ã‚¹ã‚¯ã€‘
        {task}
        
        ã€è©•ä¾¡å¯¾è±¡ã®å›ç­”ã€‘
        {response}
        
        ä»¥ä¸‹ã®å½¢å¼ã§è©•ä¾¡çµæœã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
        
        ## ç·åˆè©•ä¾¡
        - ç·åˆã‚¹ã‚³ã‚¢: X/10ç‚¹
        - å“è³ªãƒ¬ãƒ™ãƒ«: [å„ªç§€/è‰¯å¥½/æ™®é€š/è¦æ”¹å–„/ä¸ååˆ†]
        
        ## è©³ç´°è©•ä¾¡
        {criteria_text}
        
        å„é …ç›®ã«ã¤ã„ã¦ï¼š
        - ã‚¹ã‚³ã‚¢: X/10ç‚¹
        - ã‚³ãƒ¡ãƒ³ãƒˆ: å…·ä½“çš„ãªè©•ä¾¡ç†ç”±
        
        ## è‰¯ã„ç‚¹
        - è©•ä¾¡ã§ãã‚‹ç‚¹ã‚’å…·ä½“çš„ã«åˆ—æŒ™
        
        ## æ”¹å–„ç‚¹
        - å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’åˆ—æŒ™
        - å„ªå…ˆåº¦ã®é«˜ã„æ”¹å–„ç‚¹ã‹ã‚‰é †ã«è¨˜è¼‰
        
        ## æ¬¡å›ã¸ã®ææ¡ˆ
        - ã‚ˆã‚Šè‰¯ã„å›ç­”ã«ã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹
        
        å³æ ¼ã‹ã¤å»ºè¨­çš„ãªè©•ä¾¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
        """

        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt),
            ]

            response_obj = self.llm.invoke(messages)
            execution_time = time.time() - start_time

            # ===== ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ãªæ–¹æ³•ï¼‰ =====
            evaluation_text = response_obj.content
            overall_score = self._extract_overall_score(evaluation_text)
            needs_improvement = overall_score < quality_threshold

            print(f"ğŸ“Š Evaluator: è©•ä¾¡å®Œäº† - ã‚¹ã‚³ã‚¢: {overall_score}/10")
            if needs_improvement:
                print(f"âš ï¸ å“è³ªé–¾å€¤({quality_threshold})ã‚’ä¸‹å›ã£ã¦ã„ã‚‹ãŸã‚æ”¹å–„ãŒå¿…è¦")
            else:
                print(f"âœ… å“è³ªé–¾å€¤({quality_threshold})ã‚’æº€ãŸã—ã¦ã„ã¾ã™")

            return {
                "success": True,
                "evaluation_text": evaluation_text,
                "overall_score": overall_score,
                "needs_improvement": needs_improvement,
                "quality_threshold": quality_threshold,
                "evaluation_number": self.evaluation_count,
                "execution_time": execution_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ Evaluator: è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")

            return {
                "success": False,
                "error": str(e),
                "evaluation_number": self.evaluation_count,
                "execution_time": execution_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

    def _extract_overall_score(self, evaluation_text: str) -> float:
        """
        è©•ä¾¡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç·åˆã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º

        Args:
            evaluation_text (str): è©•ä¾¡ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            float: æŠ½å‡ºã•ã‚ŒãŸã‚¹ã‚³ã‚¢ï¼ˆæŠ½å‡ºã§ããªã„å ´åˆã¯5.0ï¼‰
        """

        import re

        # ===== ã€Œç·åˆã‚¹ã‚³ã‚¢: X/10ç‚¹ã€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ =====
        score_patterns = [
            r"ç·åˆã‚¹ã‚³ã‚¢[ï¼š:]\s*(\d+(?:\.\d+)?)[/ï¼]10",
            r"ç·åˆ[ï¼š:].*?(\d+(?:\.\d+)?)[/ï¼]10",
            r"(\d+(?:\.\d+)?)[/ï¼]10ç‚¹",
            r"ã‚¹ã‚³ã‚¢[ï¼š:]\s*(\d+(?:\.\d+)?)",
        ]

        for pattern in score_patterns:
            match = re.search(pattern, evaluation_text)
            if match:
                try:
                    score = float(match.group(1))
                    return min(max(score, 0), 10)  # 0-10ã®ç¯„å›²ã«åˆ¶é™
                except ValueError:
                    continue

        # ===== ã‚¹ã‚³ã‚¢ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ =====
        print("âš ï¸ ç·åˆã‚¹ã‚³ã‚¢ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ 5.0 ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return 5.0


class EvaluatorOptimizer:
    """
    è©•ä¾¡è€…ãƒ»æœ€é©åŒ–è€…ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, max_iterations: int = 3, quality_threshold: float = 7.0):
        self.generator = Generator()
        self.evaluator = Evaluator()
        self.max_iterations = max_iterations
        self.quality_threshold = quality_threshold
        self.optimization_log = []

    def optimize_response(
        self, task: str, evaluation_criteria: Dict[str, str], context: str = ""
    ) -> Dict[str, Any]:
        """
        è©•ä¾¡ãƒ»æœ€é©åŒ–ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ

        Args:
            task (str): å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯
            evaluation_criteria (Dict[str, str]): è©•ä¾¡åŸºæº–
            context (str): è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±

        Returns:
            Dict[str, Any]: æœ€é©åŒ–çµæœ
        """

        start_time = time.time()
        print("ğŸš€ è©•ä¾¡ãƒ»æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")
        print(
            f"ğŸ“‹ æœ€å¤§åå¾©å›æ•°: {self.max_iterations}, å“è³ªé–¾å€¤: {self.quality_threshold}/10"
        )
        print("-" * 60)

        iterations = []
        current_response = None
        best_response = None
        best_score = 0

        for iteration in range(1, self.max_iterations + 1):
            print(f"\nğŸ”„ åå¾© {iteration}/{self.max_iterations}")
            print("-" * 30)

            iteration_start = time.time()

            # ===== ã‚¹ãƒ†ãƒƒãƒ—1: å›ç­”ç”Ÿæˆ =====
            if iteration == 1:
                # ===== åˆå›ç”Ÿæˆ =====
                generation_result = self.generator.generate_initial_response(
                    task, context
                )
            else:
                # ===== æ”¹å–„ç”Ÿæˆ =====
                previous_evaluation = iterations[-1]["evaluation"]
                generation_result = self.generator.generate_improved_response(
                    task,
                    current_response,
                    previous_evaluation["evaluation_text"],
                    context,
                )

            if not generation_result["success"]:
                print(f"âŒ åå¾© {iteration}: ç”Ÿæˆå¤±æ•—")
                break

            current_response = generation_result["content"]

            # ===== ã‚¹ãƒ†ãƒƒãƒ—2: è©•ä¾¡ =====
            evaluation_result = self.evaluator.evaluate_response(
                task, current_response, evaluation_criteria, self.quality_threshold
            )

            if not evaluation_result["success"]:
                print(f"âŒ åå¾© {iteration}: è©•ä¾¡å¤±æ•—")
                break

            current_score = evaluation_result["overall_score"]

            # ===== æœ€è‰¯å›ç­”ã®æ›´æ–° =====
            if current_score > best_score:
                best_response = current_response
                best_score = current_score
                print(f"ğŸ† æ–°ã—ã„æœ€è‰¯å›ç­”ã‚’è¨˜éŒ²: {best_score}/10")

            # ===== åå¾©çµæœã‚’è¨˜éŒ² =====
            iteration_time = time.time() - iteration_start
            iteration_data = {
                "iteration": iteration,
                "generation": generation_result,
                "evaluation": evaluation_result,
                "response": current_response,
                "score": current_score,
                "is_best": current_score == best_score,
                "iteration_time": iteration_time,
            }
            iterations.append(iteration_data)

            print(
                f"ğŸ“Š åå¾© {iteration} å®Œäº† - ã‚¹ã‚³ã‚¢: {current_score}/10 ({iteration_time:.2f}ç§’)"
            )

            # ===== å“è³ªé–¾å€¤ã«é”ã—ãŸå ´åˆã¯çµ‚äº† =====
            if not evaluation_result["needs_improvement"]:
                print(
                    f"ğŸ‰ å“è³ªé–¾å€¤ {self.quality_threshold}/10 ã«é”ã—ã¾ã—ãŸï¼æœ€é©åŒ–å®Œäº†"
                )
                break

        total_time = time.time() - start_time

        # ===== æœ€çµ‚çµæœã‚’ã¾ã¨ã‚ã‚‹ =====
        result = {
            "task": task,
            "context": context,
            "evaluation_criteria": evaluation_criteria,
            "max_iterations": self.max_iterations,
            "quality_threshold": self.quality_threshold,
            "completed_iterations": len(iterations),
            "iterations": iterations,
            "best_response": best_response,
            "best_score": best_score,
            "final_response": current_response,
            "final_score": iterations[-1]["score"] if iterations else 0,
            "improvement_achieved": best_score > iterations[0]["score"]
            if iterations
            else False,
            "threshold_reached": best_score >= self.quality_threshold,
            "total_execution_time": total_time,
        }

        # ===== ãƒ­ã‚°ã«è¨˜éŒ² =====
        self.optimization_log.append(
            {
                "task_summary": task[:100] + "..." if len(task) > 100 else task,
                "completed_iterations": len(iterations),
                "best_score": best_score,
                "improvement": best_score - iterations[0]["score"] if iterations else 0,
                "threshold_reached": best_score >= self.quality_threshold,
                "execution_time": total_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
        )

        print("\nğŸ¯ æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†")
        print(
            f"ğŸ“Š æœ€çµ‚çµæœ: {best_score}/10 (æ”¹å–„åº¦: +{best_score - iterations[0]['score']:.1f})"
        )
        print(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’")

        return result

    def translate_and_optimize(
        self, text: str, target_language: str = "è‹±èª", domain: str = "ä¸€èˆ¬"
    ) -> Dict[str, Any]:
        """
        ç¿»è¨³å“è³ªæœ€é©åŒ–ã®å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰

        Args:
            text (str): ç¿»è¨³ã™ã‚‹æ–‡ç« 
            target_language (str): ç¿»è¨³å…ˆè¨€èª
            domain (str): åˆ†é‡ï¼ˆä¸€èˆ¬ã€æŠ€è¡“ã€æ–‡å­¦ã€ãƒ“ã‚¸ãƒã‚¹ãªã©ï¼‰

        Returns:
            Dict[str, Any]: ç¿»è¨³æœ€é©åŒ–çµæœ
        """

        task = f"""
        ä»¥ä¸‹ã®æ–‡ç« ã‚’{target_language}ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
        
        ã€ç¿»è¨³å¯¾è±¡æ–‡ç« ã€‘
        {text}
        
        ã€ç¿»è¨³è¦ä»¶ã€‘
        - åˆ†é‡: {domain}
        - åŸæ–‡ã®æ„å‘³ã¨ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’æ­£ç¢ºã«ä¼ãˆã‚‹
        - {target_language}ã¨ã—ã¦è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„è¡¨ç¾
        - å°‚é–€ç”¨èªã¯é©åˆ‡ã«ç¿»è¨³
        - æ–‡åŒ–çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®
        """

        context = f"""
        ç¿»è¨³åˆ†é‡: {domain}
        å¯¾è±¡è¨€èª: {target_language}
        åŸæ–‡ã®è¨€èª: æ—¥æœ¬èª
        å“è³ªé‡è¦–ã®é«˜ç²¾åº¦ç¿»è¨³ã‚’æ±‚ã‚ã¦ã„ã¾ã™ã€‚
        """

        return self.optimize_response(
            task=task,
            evaluation_criteria=EvaluationCriteria.TRANSLATION_QUALITY,
            context=context,
        )

    def write_and_optimize(
        self, topic: str, content_type: str = "è¨˜äº‹", target_audience: str = "ä¸€èˆ¬èª­è€…"
    ) -> Dict[str, Any]:
        """
        æ–‡ç« ä½œæˆå“è³ªæœ€é©åŒ–ã®å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰

        Args:
            topic (str): åŸ·ç­†ãƒˆãƒ”ãƒƒã‚¯
            content_type (str): ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ï¼ˆè¨˜äº‹ã€ãƒ¬ãƒãƒ¼ãƒˆã€ã‚¨ãƒƒã‚»ã‚¤ãªã©ï¼‰
            target_audience (str): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…

        Returns:
            Dict[str, Any]: æ–‡ç« æœ€é©åŒ–çµæœ
        """

        task = f"""
        ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦{content_type}ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
        
        ã€ãƒˆãƒ”ãƒƒã‚¯ã€‘
        {topic}
        
        ã€åŸ·ç­†è¦ä»¶ã€‘
        - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—: {content_type}
        - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…: {target_audience}
        - èª­ã¿ã‚„ã™ãé­…åŠ›çš„ãªæ–‡ç« 
        - è«–ç†çš„ãªæ§‹æˆ
        - èª­è€…ã®é–¢å¿ƒã‚’å¼•ãå†…å®¹
        - é©åˆ‡ãªé•·ã•ï¼ˆ800-1200æ–‡å­—ç¨‹åº¦ï¼‰
        """

        context = f"""
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—: {content_type}
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…: {target_audience}
        é«˜å“è³ªã§é­…åŠ›çš„ãªæ–‡ç« ã‚’æ±‚ã‚ã¦ã„ã¾ã™ã€‚
        """

        return self.optimize_response(
            task=task,
            evaluation_criteria=EvaluationCriteria.WRITING_QUALITY,
            context=context,
        )

    def get_optimization_statistics(self) -> Dict[str, Any]:
        """
        æœ€é©åŒ–çµ±è¨ˆã‚’å–å¾—

        Returns:
            Dict[str, Any]: çµ±è¨ˆæƒ…å ±
        """

        if not self.optimization_log:
            return {"total_optimizations": 0}

        total_optimizations = len(self.optimization_log)
        total_iterations = sum(
            log["completed_iterations"] for log in self.optimization_log
        )
        successful_optimizations = sum(
            1 for log in self.optimization_log if log["threshold_reached"]
        )
        total_improvement = sum(log["improvement"] for log in self.optimization_log)
        total_time = sum(log["execution_time"] for log in self.optimization_log)

        avg_iterations = total_iterations / total_optimizations
        avg_improvement = total_improvement / total_optimizations
        avg_time = total_time / total_optimizations
        success_rate = (successful_optimizations / total_optimizations) * 100

        return {
            "total_optimizations": total_optimizations,
            "total_iterations": total_iterations,
            "successful_optimizations": successful_optimizations,
            "success_rate": success_rate,
            "average_iterations": avg_iterations,
            "average_improvement": avg_improvement,
            "average_execution_time": avg_time,
            "total_execution_time": total_time,
        }


# ===== ä½¿ç”¨ä¾‹ =====
def main():
    """
    Evaluator-optimizerãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("=== Evaluator-optimizer ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‡ãƒ¢ ===\n")

    # ===== è©•ä¾¡ãƒ»æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ =====
    optimizer = EvaluatorOptimizer(max_iterations=3, quality_threshold=7.5)

    # ===== ãƒ‡ãƒ¢1: ç¿»è¨³å“è³ªã®æœ€é©åŒ– =====
    print("ğŸŒ ãƒ‡ãƒ¢1: ç¿»è¨³å“è³ªã®æœ€é©åŒ–")
    print("=" * 50)

    japanese_text = """
    äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã«ã‚ˆã‚Šã€æˆ‘ã€…ã®ç¤¾ä¼šã¯æ ¹æœ¬çš„ãªå¤‰é©ã‚’è¿ãˆã¦ã„ã‚‹ã€‚
    æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ã®é€²æ­©ã¯ã€å¾“æ¥äººé–“ã®å°‚å£²ç‰¹è¨±ã¨ã•ã‚Œã¦ã„ãŸèªçŸ¥çš„ã‚¿ã‚¹ã‚¯ã‚’
    è‡ªå‹•åŒ–ã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã—ã€åŠ´åƒå¸‚å ´ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã€‚
    ã—ã‹ã—ã€ã“ã®æŠ€è¡“é©æ–°ã¯å˜ãªã‚‹åŠ¹ç‡åŒ–ã«ã¨ã©ã¾ã‚‰ãšã€
    äººé–“ã¨æ©Ÿæ¢°ã®æ–°ãŸãªå”åƒé–¢ä¿‚ã‚’ç¯‰ãæ©Ÿä¼šã§ã‚‚ã‚ã‚‹ã€‚
    é‡è¦ãªã®ã¯ã€æŠ€è¡“ã®ç™ºå±•ã‚’æã‚Œã‚‹ã®ã§ã¯ãªãã€
    ã„ã‹ã«ã—ã¦äººé–“ã®å‰µé€ æ€§ã¨æ©Ÿæ¢°ã®è¨ˆç®—èƒ½åŠ›ã‚’
    æœ€é©ã«çµ„ã¿åˆã‚ã›ã‚‹ã‹ã‚’è€ƒãˆã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚
    """

    translation_result = optimizer.translate_and_optimize(
        text=japanese_text, target_language="è‹±èª", domain="æŠ€è¡“ãƒ»å­¦è¡“"
    )

    print("\nğŸ“Š ç¿»è¨³æœ€é©åŒ–çµæœ:")
    print(f"- åå¾©å›æ•°: {translation_result['completed_iterations']}")
    print(f"- æœ€é«˜ã‚¹ã‚³ã‚¢: {translation_result['best_score']}/10")
    print(
        f"- æ”¹å–„åº¦: +{translation_result['best_score'] - translation_result['iterations'][0]['score']:.1f}ç‚¹"
    )
    print(
        f"- é–¾å€¤é”æˆ: {'ã¯ã„' if translation_result['threshold_reached'] else 'ã„ã„ãˆ'}"
    )
    print(f"- å®Ÿè¡Œæ™‚é–“: {translation_result['total_execution_time']:.2f}ç§’")

    print("\nğŸ† æœ€è‰¯ã®ç¿»è¨³:")
    print(
        translation_result["best_response"][:300] + "..."
        if len(translation_result["best_response"]) > 300
        else translation_result["best_response"]
    )

    # ===== ãƒ‡ãƒ¢2: æ–‡ç« ä½œæˆã®æœ€é©åŒ– =====
    print("\n\nâœï¸ ãƒ‡ãƒ¢2: æ–‡ç« ä½œæˆã®æœ€é©åŒ–")
    print("=" * 50)

    writing_result = optimizer.write_and_optimize(
        topic="ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ãŒä¼æ¥­æ–‡åŒ–ã«ä¸ãˆã‚‹å½±éŸ¿ã¨ä»Šå¾Œã®å±•æœ›",
        content_type="ãƒ“ã‚¸ãƒã‚¹è¨˜äº‹",
        target_audience="ä¼æ¥­çµŒå–¶è€…ãƒ»äººäº‹æ‹…å½“è€…",
    )

    print("\nğŸ“Š æ–‡ç« æœ€é©åŒ–çµæœ:")
    print(f"- åå¾©å›æ•°: {writing_result['completed_iterations']}")
    print(f"- æœ€é«˜ã‚¹ã‚³ã‚¢: {writing_result['best_score']}/10")
    print(
        f"- æ”¹å–„åº¦: +{writing_result['best_score'] - writing_result['iterations'][0]['score']:.1f}ç‚¹"
    )
    print(f"- é–¾å€¤é”æˆ: {'ã¯ã„' if writing_result['threshold_reached'] else 'ã„ã„ãˆ'}")
    print(f"- å®Ÿè¡Œæ™‚é–“: {writing_result['total_execution_time']:.2f}ç§’")

    print("\nğŸ† æœ€è‰¯ã®è¨˜äº‹:")
    print(
        writing_result["best_response"][:400] + "..."
        if len(writing_result["best_response"]) > 400
        else writing_result["best_response"]
    )

    # ===== ãƒ‡ãƒ¢3: ä¸€èˆ¬çš„ãªã‚¿ã‚¹ã‚¯ã®æœ€é©åŒ– =====
    print("\n\nğŸ”¬ ãƒ‡ãƒ¢3: ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã®æœ€é©åŒ–")
    print("=" * 50)

    research_task = """
    ã€Œæ—¥æœ¬ã«ãŠã‘ã‚‹ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆDXï¼‰ã®ç¾çŠ¶ã¨èª²é¡Œã€ã«ã¤ã„ã¦ã€
    ä»¥ä¸‹ã®è¦ç´ ã‚’å«ã‚€åŒ…æ‹¬çš„ãªç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
    
    1. DXã®å®šç¾©ã¨é‡è¦æ€§
    2. æ—¥æœ¬ä¼æ¥­ã®DXå°å…¥çŠ¶æ³ï¼ˆçµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
    3. ä¸»è¦ãªæˆåŠŸäº‹ä¾‹ã®åˆ†æ
    4. å°å…¥ã«ãŠã‘ã‚‹ä¸»ãªéšœå®³ã¨èª²é¡Œ
    5. æ”¿åºœã®æ”¿ç­–ã¨æ”¯æ´åˆ¶åº¦
    6. å›½éš›æ¯”è¼ƒï¼ˆç‰¹ã«ã‚¢ãƒ¡ãƒªã‚«ã€ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã¨ã®æ¯”è¼ƒï¼‰
    7. ä»Šå¾Œã®å±•æœ›ã¨æè¨€
    
    ãƒ¬ãƒãƒ¼ãƒˆã¯å­¦è¡“çš„ãªå³å¯†æ€§ã‚’ä¿ã¡ã¤ã¤ã€å®Ÿå‹™è€…ã«ã‚‚ç†è§£ã—ã‚„ã™ã„å†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚
    """

    research_result = optimizer.optimize_response(
        task=research_task,
        evaluation_criteria=EvaluationCriteria.RESEARCH_QUALITY,
        context="å­¦è¡“è«–æ–‡ãƒ¬ãƒ™ãƒ«ã®è³ªã‚’æ±‚ã‚ã‚‹åŒ…æ‹¬çš„ãªç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆ",
    )

    print("\nğŸ“Š ç ”ç©¶ãƒ¬ãƒãƒ¼ãƒˆæœ€é©åŒ–çµæœ:")
    print(f"- åå¾©å›æ•°: {research_result['completed_iterations']}")
    print(f"- æœ€é«˜ã‚¹ã‚³ã‚¢: {research_result['best_score']}/10")
    print(
        f"- æ”¹å–„åº¦: +{research_result['best_score'] - research_result['iterations'][0]['score']:.1f}ç‚¹"
    )
    print(f"- é–¾å€¤é”æˆ: {'ã¯ã„' if research_result['threshold_reached'] else 'ã„ã„ãˆ'}")
    print(f"- å®Ÿè¡Œæ™‚é–“: {research_result['total_execution_time']:.2f}ç§’")

    print("\nğŸ† æœ€è‰¯ã®ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(
        research_result["best_response"][:500] + "..."
        if len(research_result["best_response"]) > 500
        else research_result["best_response"]
    )

    # ===== çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º =====
    print("\n\nğŸ“ˆ æœ€é©åŒ–çµ±è¨ˆ")
    print("=" * 30)
    stats = optimizer.get_optimization_statistics()
    print(f"ç·æœ€é©åŒ–å›æ•°: {stats['total_optimizations']}")
    print(f"ç·åå¾©å›æ•°: {stats['total_iterations']}")
    print(f"æˆåŠŸã—ãŸæœ€é©åŒ–: {stats['successful_optimizations']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.1f}%")
    print(f"å¹³å‡åå¾©å›æ•°: {stats['average_iterations']:.1f}")
    print(f"å¹³å‡æ”¹å–„åº¦: +{stats['average_improvement']:.1f}ç‚¹")
    print(f"å¹³å‡å®Ÿè¡Œæ™‚é–“: {stats['average_execution_time']:.2f}ç§’")
    print(f"ç·å®Ÿè¡Œæ™‚é–“: {stats['total_execution_time']:.2f}ç§’")


if __name__ == "__main__":
    main()
