#!/usr/bin/env python3
"""
LangGraphä¾‹5: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹

ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã®å®Ÿè£…ã¨ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
"""
import asyncio
import time
import random
from typing import AsyncGenerator, Dict, Any

from langgraph import StateGraph
from langgraph.checkpoint.sqlite import SqliteSaver

from ..core.state import ChatState, Message, ConversationRole
from ..streaming.stream_processor import (
    StreamingChatDisplay, 
    StreamProcessor,
    StreamChunk
)


def create_initial_state(user_input: str) -> ChatState:
    """åˆæœŸçŠ¶æ…‹ã®ä½œæˆ"""
    return {
        "messages": [],
        "current_step": "start",
        "iteration_count": 0,
        "error_count": 0,
        "last_error": None,
        "metadata": {},
        "user_input": user_input,
        "ai_response": "",
        "conversation_id": "streaming_example",
        "session_active": True,
    }


async def simulate_llm_stream(prompt: str) -> AsyncGenerator[str, None]:
    """LLMã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    # å®Ÿéš›ã®LLMã®ä»£ã‚ã‚Šã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    responses = {
        "ã“ã‚“ã«ã¡ã¯": [
            "ã“ã‚“ã«ã¡ã¯ï¼", "ä»Šæ—¥ã¯", "ã¨ã¦ã‚‚", "è‰¯ã„", "å¤©æ°—", "ã§ã™ã­ã€‚",
            "ä½•ã‹", "ãŠæ‰‹ä¼ã„", "ã§ãã‚‹", "ã“ã¨ã¯", "ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        ],
        "langgraph": [
            "LangGraph", "ã¯", "è¤‡é›‘ãª", "AI", "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼", "ã‚’", "æ§‹ç¯‰", "ã™ã‚‹ãŸã‚ã®",
            "å¼·åŠ›ãª", "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯", "ã§ã™ã€‚", "ã‚¹ãƒ†ãƒ¼ãƒˆ", "ã‚°ãƒ©ãƒ•", "ã‚’", "ä½¿ç”¨", "ã—ã¦",
            "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ", "ã®", "å‹•ä½œ", "ã‚’", "åˆ¶å¾¡", "ã§ãã¾ã™ã€‚"
        ],
        "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°": [
            "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°", "å‡¦ç†", "ã«ã‚ˆã‚Šã€", "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "ã¯", "AI", "ã®", "å›ç­”", "ã‚’",
            "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ", "ã§", "ç¢ºèª", "ã§ãã¾ã™ã€‚", "ã“ã‚Œã«ã‚ˆã‚Š", "ã‚ˆã‚Š", "è‡ªç„¶", "ãª",
            "å¯¾è©±", "ä½“é¨“", "ãŒ", "å¯èƒ½", "ã«", "ãªã‚Šã¾ã™ã€‚"
        ]
    }
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
    words = None
    for keyword, word_list in responses.items():
        if keyword in prompt.lower():
            words = word_list
            break
    
    if words is None:
        words = ["ç”³ã—è¨³", "ã‚ã‚Šã¾ã›ã‚“", "ãŒã€", "ãã®", "å†…å®¹", "ã«ã¤ã„ã¦", "è©³ã—ã", 
                "èª¬æ˜", "ã§ãã¾ã›ã‚“ã€‚", "ä»–ã®", "è³ªå•", "ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"]
    
    # å„å˜èªã‚’é †æ¬¡yieldï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
    for word in words:
        # ãƒ©ãƒ³ãƒ€ãƒ ãªé…å»¶ã§ã‚ˆã‚Šãƒªã‚¢ãƒ«ã«
        await asyncio.sleep(random.uniform(0.1, 0.3))
        yield word


def user_input_node(state: ChatState) -> Dict[str, Any]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å‡¦ç†ãƒãƒ¼ãƒ‰"""
    print(f"\nğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {state['user_input']}")
    
    user_message: Message = {
        "role": ConversationRole.HUMAN,
        "content": state["user_input"],
        "timestamp": None,
        "metadata": None,
    }
    
    return {
        "messages": state["messages"] + [user_message],
        "current_step": "streaming",
        "iteration_count": state["iteration_count"] + 1,
    }


async def streaming_ai_response_node(state: ChatState) -> Dict[str, Any]:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°AIå¿œç­”ãƒãƒ¼ãƒ‰"""
    print(f"\nğŸ”„ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹...")
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–
    chat_display = StreamingChatDisplay()
    
    # LLMã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    response_stream = simulate_llm_stream(state["user_input"])
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºå®Ÿè¡Œ
    full_response = await chat_display.display_streaming_response(
        response_stream,
        title="ğŸ¤– AIå¿œç­”ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"
    )
    
    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    chat_display.display_statistics()
    
    # AIå¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    ai_message: Message = {
        "role": ConversationRole.ASSISTANT,
        "content": full_response,
        "timestamp": None,
        "metadata": {
            "streaming": True,
            "response_length": len(full_response),
        }
    }
    
    return {
        "messages": state["messages"] + [ai_message],
        "ai_response": full_response,
        "current_step": "completed",
    }


def should_continue(state: ChatState) -> str:
    """ç¶™ç¶šåˆ¤å®šé–¢æ•°"""
    return "end"


async def demonstrate_buffer_management():
    """ãƒãƒƒãƒ•ã‚¡ç®¡ç†ã®ãƒ‡ãƒ¢"""
    print("\n" + "="*50)
    print("ğŸ”§ ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒƒãƒ•ã‚¡ç®¡ç†ã®ãƒ‡ãƒ¢")
    print("="*50)
    
    processor = StreamProcessor(buffer_size=5)  # å°ã•ãªãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    
    async def test_stream():
        """ãƒ†ã‚¹ãƒˆç”¨ã‚¹ãƒˆãƒªãƒ¼ãƒ """
        test_chunks = [f"ãƒãƒ£ãƒ³ã‚¯{i} " for i in range(10)]
        for chunk in test_chunks:
            yield chunk
            await asyncio.sleep(0.1)
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†å®Ÿè¡Œ
    result = await processor.process_stream(
        test_stream(),
        display_progress=True
    )
    
    print(f"\nğŸ“Š å‡¦ç†çµæœ:")
    print(f"  - å®Œå…¨ãªå†…å®¹: {result}")
    print(f"  - ãƒãƒƒãƒ•ã‚¡å†…å®¹: {processor.get_buffer_content()}")
    
    stats = processor.get_chunk_statistics()
    print(f"  - çµ±è¨ˆ: {stats}")
    
    # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
    processor.clear_buffer()
    print(f"  - ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢å¾Œ: '{processor.get_buffer_content()}'")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 50)
    print("ğŸ¯ ä¾‹5: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
    print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã¨ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°")
    print("=" * 50)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š
    checkpointer = SqliteSaver.from_conn_string(":memory:")
    
    # ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
    workflow = StateGraph(ChatState)
    
    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
    workflow.add_node("user_input", user_input_node)
    workflow.add_node("streaming_response", streaming_ai_response_node)
    
    # ã‚¨ãƒƒã‚¸ã®è¨­å®š
    workflow.add_edge("user_input", "streaming_response")
    workflow.add_conditional_edges(
        "streaming_response",
        should_continue,
        {"end": "__end__"}
    )
    
    # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆè¨­å®š
    workflow.set_entry_point("user_input")
    
    # ã‚°ãƒ©ãƒ•ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    app = workflow.compile(checkpointer=checkpointer)
    
    # å®Ÿè¡Œä¾‹
    test_inputs = [
        "ã“ã‚“ã«ã¡ã¯",
        "LangGraphã«ã¤ã„ã¦æ•™ãˆã¦",
        "ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã®ä»•çµ„ã¿ã¯ï¼Ÿ"
    ]
    
    for i, user_input in enumerate(test_inputs, 1):
        print(f"\n{'='*20} å®Ÿè¡Œä¾‹ {i} {'='*20}")
        
        initial_state = create_initial_state(user_input)
        thread_id = f"streaming_thread_{i}"
        config = {"configurable": {"thread_id": thread_id}}
        
        # å‡¦ç†æ™‚é–“è¨ˆæ¸¬
        start_time = time.time()
        result = await app.ainvoke(initial_state, config)
        end_time = time.time()
        
        print(f"\nâ±ï¸  å‡¦ç†æ™‚é–“: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ“ˆ æœ€çµ‚çŠ¶æ…‹: {result['current_step']}")
    
    # ãƒãƒƒãƒ•ã‚¡ç®¡ç†ã®ãƒ‡ãƒ¢
    await demonstrate_buffer_management()
    
    print(f"\nâœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ä¾‹ãŒå®Œäº†ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    asyncio.run(main())